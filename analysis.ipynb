{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä High Valyrian NLP ‚Äî Tokenization & Embedding Analysis\n",
    "\n",
    "**Tugas Individu Week 3 ‚Äî NLP, Pradita University**\n",
    "\n",
    "Notebook ini menjalankan seluruh pipeline secara terpadu:\n",
    "1. **Text Cleaning** ‚Äî normalisasi dan pembersihan corpus\n",
    "2. **Custom Tokenizer** ‚Äî word-level tokenization\n",
    "3. **Vocabulary & Token ID Mapping** ‚Äî frequency-based vocab\n",
    "4. **Word Embedding** ‚Äî PPMI + SVD\n",
    "5. **Quantitative Analysis** ‚Äî statistik, visualisasi, dan evaluasi embedding\n",
    "\n",
    "**Corpus**: [High Valyrian (Game of Thrones) ‚Äî Kaggle](https://www.kaggle.com/datasets/viceriomarinowski/high-valyrian-corpus-from-game-of-thrones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "BASE = Path('.')\n",
    "print('‚úÖ Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Text Cleaning\n",
    "\n",
    "Membaca corpus mentah dan menerapkan langkah-langkah pembersihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cleaning script\n",
    "exec(open('clean_corpus.py', encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview cleaned corpus\n",
    "cleaned = Path('cleaned_corpus.txt').read_text(encoding='utf-8')\n",
    "cleaned_lines = [l for l in cleaned.split('\\n') if l.strip()]\n",
    "print(f'Total baris: {len(cleaned_lines)}')\n",
    "print(f'Total kata:  {len(cleaned.split())}')\n",
    "print()\n",
    "print('--- 5 baris pertama ---')\n",
    "for line in cleaned_lines[:5]:\n",
    "    print(f'  {line[:100]}...' if len(line) > 100 else f'  {line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Custom Tokenizer\n",
    "\n",
    "Word-level tokenizer yang dibuat dari scratch tanpa library NLP:\n",
    "- Punctuation dipisah sebagai token tersendiri\n",
    "- Setiap baris dibungkus `<BOS>` (Begin of Sentence) dan `<EOS>` (End of Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'([.!?,;:])', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_line(line: str) -> list[str]:\n",
    "    tokens = tokenize(line)\n",
    "    return ['<BOS>'] + tokens + ['<EOS>'] if tokens else []\n",
    "\n",
    "# Tokenize all lines\n",
    "all_tokenized = [tokenize_line(l) for l in cleaned_lines]\n",
    "all_tokenized = [t for t in all_tokenized if t]\n",
    "all_tokens_flat = [tok for line in all_tokenized for tok in line]\n",
    "\n",
    "print(f'Lines tokenized: {len(all_tokenized)}')\n",
    "print(f'Total tokens:    {len(all_tokens_flat)}')\n",
    "print()\n",
    "print('--- Contoh tokenisasi ---')\n",
    "for i in range(3):\n",
    "    print(f'  Line {i+1}: {all_tokenized[i][:8]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Vocabulary & Token ID Mapping\n",
    "\n",
    "Membangun vocabulary yang diurutkan berdasarkan frekuensi (descending).\n",
    "Special tokens: `<PAD>=0`, `<UNK>=1`, `<BOS>=2`, `<EOS>=3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
    "\n",
    "freq = Counter(all_tokens_flat)\n",
    "sorted_tokens = sorted(\n",
    "    [t for t in freq if t not in SPECIAL_TOKENS],\n",
    "    key=lambda t: (-freq[t], t)\n",
    ")\n",
    "\n",
    "vocab = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
    "for i, tok in enumerate(sorted_tokens):\n",
    "    vocab[tok] = len(SPECIAL_TOKENS) + i\n",
    "\n",
    "def encode(tokens, vocab):\n",
    "    unk_id = vocab['<UNK>']\n",
    "    return [vocab.get(t, unk_id) for t in tokens]\n",
    "\n",
    "def decode(ids, vocab):\n",
    "    id2tok = {v: k for k, v in vocab.items()}\n",
    "    return [id2tok.get(i, '<UNK>') for i in ids]\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')\n",
    "print(f'  Special:  {len(SPECIAL_TOKENS)}')\n",
    "print(f'  Words:    {len(vocab) - len(SPECIAL_TOKENS)}')\n",
    "print()\n",
    "# Sanity check\n",
    "sample = all_tokenized[0]\n",
    "enc = encode(sample, vocab)\n",
    "dec = decode(enc, vocab)\n",
    "print(f'Encode ‚Üí Decode match: {sample == dec} ‚úÖ' if sample == dec else '‚ùå MISMATCH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Distribusi Frekuensi Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 tokens (excluding special)\n",
    "word_freq = {t: c for t, c in freq.items() if t not in SPECIAL_TOKENS and t not in '.,?!;:'}\n",
    "top20 = sorted(word_freq.items(), key=lambda x: -x[1])[:20]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "tokens_list = [t[0] for t in top20]\n",
    "counts_list = [t[1] for t in top20]\n",
    "bars = ax.barh(tokens_list[::-1], counts_list[::-1],\n",
    "               color=plt.cm.plasma(np.linspace(0.2, 0.8, 20)))\n",
    "ax.set_xlabel('Frekuensi', fontsize=11)\n",
    "ax.set_title('Top 20 Token Paling Sering', fontsize=14, fontweight='bold')\n",
    "for bar, count in zip(bars, counts_list[::-1]):\n",
    "    ax.text(bar.get_width() + 10, bar.get_y() + bar.get_height()/2,\n",
    "            str(count), va='center', fontsize=9, color='#aaa')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top20_tokens.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Zipf's Law Check\n",
    "\n",
    "Hukum Zipf: frekuensi token ke-r ‚âà 1/r. Log-log plot seharusnya mendekati garis lurus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq = sorted(freq.values(), reverse=True)\n",
    "ranks = np.arange(1, len(sorted_freq) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.loglog(ranks, sorted_freq, 'o-', color='#7c5bf5', markersize=3, alpha=0.8)\n",
    "ax.set_xlabel('Rank (log)', fontsize=11)\n",
    "ax.set_ylabel('Frequency (log)', fontsize=11)\n",
    "ax.set_title(\"Zipf's Law ‚Äî Token Frequency vs Rank\", fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('zipf_law.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Word Embedding (PPMI + SVD)\n",
    "\n",
    "Membangun word embedding dari scratch:\n",
    "1. **Co-occurrence matrix** ‚Äî menghitung kemunculan bersama dalam context window\n",
    "2. **PPMI** ‚Äî Positive Pointwise Mutual Information, mengurangi bias kata frequent\n",
    "3. **SVD** ‚Äî Singular Value Decomposition, mereduksi dimensi\n",
    "\n",
    "### Kenapa 50 dimensi?\n",
    "\n",
    "| Aspek | Penjelasan |\n",
    "|---|---|\n",
    "| Rule of thumb | dim ‚âà ‚àövocab ‚âà ‚àö299 ‚âà 17, dinaikkan ke 50 untuk ekspresivitas |\n",
    "| Overfitting | >100 dim dengan 299 vocab = terlalu banyak parameter vs data |\n",
    "| SVD decay | Singular values turun cepat; komponen setelah ~50 hanya noise |\n",
    "| Standar | GloVe/Word2Vec pakai 100-300 dim untuk vocab 100K+; kita proporsional |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 50\n",
    "WINDOW_SIZE = 4\n",
    "\n",
    "# Corpus tokens (tanpa BOS/EOS untuk embedding)\n",
    "corpus_tokens = []\n",
    "for line in cleaned_lines:\n",
    "    corpus_tokens.extend(tokenize(line))\n",
    "\n",
    "print(f'Corpus tokens: {len(corpus_tokens)}')\n",
    "print(f'Vocab size:    {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "cooc = np.zeros((V, V), dtype=np.float64)\n",
    "\n",
    "for i, token in enumerate(corpus_tokens):\n",
    "    if token not in vocab:\n",
    "        continue\n",
    "    idx_i = vocab[token]\n",
    "    start = max(0, i - WINDOW_SIZE)\n",
    "    end = min(len(corpus_tokens), i + WINDOW_SIZE + 1)\n",
    "    for j in range(start, end):\n",
    "        if j == i:\n",
    "            continue\n",
    "        ctx = corpus_tokens[j]\n",
    "        if ctx not in vocab:\n",
    "            continue\n",
    "        cooc[idx_i][vocab[ctx]] += 1.0 / abs(i - j)\n",
    "\n",
    "non_zero = np.count_nonzero(cooc)\n",
    "sparsity = 1.0 - non_zero / (V * V)\n",
    "print(f'Matrix shape:   {cooc.shape}')\n",
    "print(f'Non-zero cells: {non_zero}')\n",
    "print(f'Sparsity:       {sparsity:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PPMI Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = cooc.sum()\n",
    "row_sum = cooc.sum(axis=1, keepdims=True)\n",
    "col_sum = cooc.sum(axis=0, keepdims=True)\n",
    "row_sum[row_sum == 0] = 1\n",
    "col_sum[col_sum == 0] = 1\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    pmi = np.log2(cooc * total / (row_sum * col_sum))\n",
    "pmi[~np.isfinite(pmi)] = 0\n",
    "ppmi = np.maximum(pmi, 0)\n",
    "\n",
    "print(f'PPMI max:  {ppmi.max():.4f}')\n",
    "print(f'PPMI mean: {ppmi.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SVD ‚Üí 50 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(ppmi, full_matrices=False)\n",
    "k = min(EMBED_DIM, len(S))\n",
    "embeddings = U[:, :k] * np.sqrt(S[:k])\n",
    "\n",
    "print(f'Embedding shape: {embeddings.shape}')\n",
    "print(f'File size:       {embeddings.nbytes / 1024:.1f} KB')\n",
    "\n",
    "# Save\n",
    "np.save('embedding.npy', embeddings)\n",
    "print('\\n‚úÖ Saved embedding.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Singular Value Decay\n",
    "\n",
    "Menunjukkan bahwa komponen setelah ~50 sudah mendekati 0 (noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Singular values\n",
    "ax1.plot(S[:100], 'o-', markersize=3, color='#7c5bf5')\n",
    "ax1.axvline(x=EMBED_DIM, color='#ef4444', linestyle='--', alpha=0.7, label=f'k={EMBED_DIM}')\n",
    "ax1.set_xlabel('Component')\n",
    "ax1.set_ylabel('Singular Value')\n",
    "ax1.set_title('Singular Value Decay', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.2)\n",
    "\n",
    "# Cumulative explained variance\n",
    "explained = np.cumsum(S**2) / np.sum(S**2) * 100\n",
    "ax2.plot(explained[:100], '-', color='#10b981', linewidth=2)\n",
    "ax2.axvline(x=EMBED_DIM, color='#ef4444', linestyle='--', alpha=0.7, label=f'k={EMBED_DIM}')\n",
    "ax2.axhline(y=explained[EMBED_DIM-1], color='#f59e0b', linestyle=':', alpha=0.5,\n",
    "            label=f'{explained[EMBED_DIM-1]:.1f}% variance captured')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Variance (%)')\n",
    "ax2.set_title('Cumulative Explained Variance', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('svd_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'\\n50 komponen menangkap {explained[49]:.1f}% dari total variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Analisis Kuantitatif Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Cosine Similarity ‚Äî Kata Paling Mirip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    na, nb = np.linalg.norm(a), np.linalg.norm(b)\n",
    "    return float(np.dot(a, b) / (na * nb)) if na and nb else 0.0\n",
    "\n",
    "def top_similar(word, k=8):\n",
    "    if word not in vocab: return []\n",
    "    vec = embeddings[vocab[word]]\n",
    "    sims = [(t, cosine_sim(vec, embeddings[i])) for t, i in vocab.items() if t != word]\n",
    "    return sorted(sims, key=lambda x: -x[1])[:k]\n",
    "\n",
    "probe_words = ['se', 'zaldrƒ´zoti', 'dƒÅrys', 'jorrƒÅelagon', 'perzys',\n",
    "               'henujagon', 'dƒÅervon', 'gaomagon', 'issa', 'skoros']\n",
    "\n",
    "for word in probe_words:\n",
    "    similar = top_similar(word, k=5)\n",
    "    print(f\"\\n  '{word}' (ID {vocab[word]})\")\n",
    "    for t, s in similar:\n",
    "        print(f'    ‚Üí {t:<22} sim={s:+.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Similarity Heatmap\n",
    "\n",
    "Heatmap cosine similarity antara top-30 kata (by frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 words\n",
    "top30_words = [t[0] for t in sorted(word_freq.items(), key=lambda x: -x[1])[:30]]\n",
    "top30_idx = [vocab[w] for w in top30_words]\n",
    "top30_vecs = embeddings[top30_idx]\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "norms = np.linalg.norm(top30_vecs, axis=1, keepdims=True)\n",
    "norms[norms == 0] = 1\n",
    "normed = top30_vecs / norms\n",
    "sim_matrix = normed @ normed.T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(sim_matrix, cmap='magma', vmin=-0.2, vmax=1.0)\n",
    "ax.set_xticks(range(len(top30_words)))\n",
    "ax.set_yticks(range(len(top30_words)))\n",
    "ax.set_xticklabels(top30_words, rotation=60, ha='right', fontsize=8)\n",
    "ax.set_yticklabels(top30_words, fontsize=8)\n",
    "ax.set_title('Cosine Similarity Heatmap (Top 30 Words)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('similarity_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Embedding Visualization (PCA 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to 2D\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Categorize tokens\n",
    "punct_set = {'.', ',', '?', '!', ';', ':'}\n",
    "special_set = {'<PAD>', '<UNK>', '<BOS>', '<EOS>'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "for token, idx in vocab.items():\n",
    "    x, y = emb_2d[idx]\n",
    "    f = freq.get(token, 0)\n",
    "\n",
    "    if token in special_set:\n",
    "        ax.scatter(x, y, c='#ef4444', s=80, zorder=5, edgecolors='white', linewidth=0.5)\n",
    "        ax.annotate(token, (x, y), fontsize=7, color='#ef4444', fontweight='bold',\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    elif token in punct_set:\n",
    "        ax.scatter(x, y, c='#3b82f6', s=60, zorder=4, marker='s')\n",
    "        ax.annotate(token, (x, y), fontsize=8, color='#3b82f6',\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    elif f >= 100:\n",
    "        size = min(20 + f / 10, 120)\n",
    "        ax.scatter(x, y, c='#10b981', s=size, alpha=0.7, zorder=3)\n",
    "        ax.annotate(token, (x, y), fontsize=7, color='#aaa',\n",
    "                    xytext=(5, 3), textcoords='offset points')\n",
    "    else:\n",
    "        ax.scatter(x, y, c='#555', s=12, alpha=0.4, zorder=1)\n",
    "\n",
    "ax.set_title('Word Embedding ‚Äî PCA 2D Projection', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.grid(True, alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_embedding.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Embedding Visualization (t-SNE 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE for better cluster separation\n",
    "tsne = TSNE(n_components=2, perplexity=15, random_state=42, n_iter=1000)\n",
    "emb_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "for token, idx in vocab.items():\n",
    "    x, y = emb_tsne[idx]\n",
    "    f = freq.get(token, 0)\n",
    "\n",
    "    if token in special_set:\n",
    "        ax.scatter(x, y, c='#ef4444', s=80, zorder=5, edgecolors='white', linewidth=0.5)\n",
    "        ax.annotate(token, (x, y), fontsize=7, color='#ef4444', fontweight='bold',\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    elif token in punct_set:\n",
    "        ax.scatter(x, y, c='#3b82f6', s=60, zorder=4, marker='s')\n",
    "        ax.annotate(token, (x, y), fontsize=8, color='#3b82f6',\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    elif f >= 100:\n",
    "        size = min(20 + f / 10, 120)\n",
    "        ax.scatter(x, y, c='#10b981', s=size, alpha=0.7, zorder=3)\n",
    "        ax.annotate(token, (x, y), fontsize=7, color='#aaa',\n",
    "                    xytext=(5, 3), textcoords='offset points')\n",
    "    else:\n",
    "        ax.scatter(x, y, c='#555', s=12, alpha=0.4, zorder=1)\n",
    "\n",
    "ax.set_title('Word Embedding ‚Äî t-SNE 2D Projection', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_embedding.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Ringkasan & Statistik Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 55)\n",
    "print('RINGKASAN PIPELINE')\n",
    "print('=' * 55)\n",
    "print()\n",
    "print(f'üìÑ Corpus')\n",
    "print(f'   Baris bersih:     {len(cleaned_lines)}')\n",
    "print(f'   Total kata:       {len(cleaned.split())}')\n",
    "print()\n",
    "print(f'üî§ Tokenizer')\n",
    "print(f'   Total tokens:     {len(all_tokens_flat)}')\n",
    "print(f'   Unique tokens:    {len(set(all_tokens_flat))}')\n",
    "print()\n",
    "print(f'üìñ Vocabulary')\n",
    "print(f'   Vocab size:       {len(vocab)}')\n",
    "print(f'   Special tokens:   {len(SPECIAL_TOKENS)}')\n",
    "print(f'   Unique words:     {len(vocab) - len(SPECIAL_TOKENS)}')\n",
    "print()\n",
    "print(f'üßÆ Embedding')\n",
    "print(f'   Metode:           PPMI + SVD')\n",
    "print(f'   Dimensi:          {EMBED_DIM}')\n",
    "print(f'   Matrix shape:     {embeddings.shape}')\n",
    "print(f'   Context window:   {WINDOW_SIZE}')\n",
    "print(f'   File size:        {embeddings.nbytes / 1024:.1f} KB')\n",
    "print()\n",
    "print(f'üìä Co-occurrence')\n",
    "print(f'   Non-zero cells:   {non_zero}')\n",
    "print(f'   Sparsity:         {sparsity:.1%}')\n",
    "print(f'   Variance (50 PC): {explained[49]:.1f}%')\n",
    "print()\n",
    "print('=' * 55)"
   ]
  }
 ]
}